\# Runbook



This document explains the alerts generated by the watcher service and how to respond to them.

The watcher monitors Nginx logs to detect failovers and high error rates in Blue/Green deployments.



\## Alert Types



\### 1. Failover Detected



\*\*Example message:\*\*



```

Failover detected: blue → green

Previous Pool: blue  

Current Pool: green  

Action Required: Check primary container health

```



\*\*Meaning:\*\*

Nginx switched traffic from the primary pool to the backup because the main app stopped responding or started returning errors.



\*\*Possible causes:\*\*



\* The container crashed or was stopped.

\* Health checks failed due to a timeout or internal error.

\* The app is overloaded and can’t respond in time.



\*\*What to do:\*\*



1\. Check container status:



&nbsp;  ```bash

&nbsp;  docker ps -a

&nbsp;  ```

2\. Inspect the failing container’s logs:



&nbsp;  ```bash

&nbsp;  docker logs app\_blue

&nbsp;  ```

3\. Restart the container if it’s down:



&nbsp;  ```bash

&nbsp;  docker start app\_blue

&nbsp;  ```

4\. After confirming it’s stable, test its `/healthz` endpoint to verify recovery:



&nbsp;  ```bash

&nbsp;  curl http://localhost:8081/healthz

&nbsp;  ```

5\. Nginx will automatically shift traffic back once it detects successful responses.



---



\### 2. High Error Rate



\*\*Example message:\*\*



```

High error rate detected: 12.50% 5xx over last 200 requests (25 errors)

Error Rate: 12.50%  

Threshold: 2%  

Action Required: Inspect logs, consider switching traffic

```



\*\*Meaning:\*\*

More than 2% of recent requests have returned server errors (HTTP 5xx).

This usually means something in the app or its dependencies is failing intermittently.



\*\*Possible causes:\*\*



\* Application bug causing crashes or 500 responses.

\* Downstream service failure (database, API, etc.).

\* Temporary overload or memory exhaustion.



\*\*What to do:\*\*



1\. Identify which pool is generating the errors:

&nbsp;  Check Nginx access logs for the `"pool"` field.



&nbsp;  ```bash

&nbsp;  cat nginx/logs/access.log | grep 5

&nbsp;  ```

2\. Check that container’s logs:



&nbsp;  ```bash

&nbsp;  docker logs app\_blue

&nbsp;  ```

3\. If errors persist, switch the active pool manually:



&nbsp;  ```bash

&nbsp;  docker stop app\_blue

&nbsp;  docker start app\_green

&nbsp;  ```

4\. Confirm that the new pool is stable and alert frequency drops.



---



\### 3. Recovery / Pool Restored



\*\*Example message:\*\*



```

Traffic restored: green → blue

Current Pool: blue  

Status: Recovered successfully

```



\*\*Meaning:\*\*

The previously failed pool recovered, and Nginx returned traffic to it.



\*\*What to do:\*\*



\* No action needed if recovery was expected.

\* Monitor logs briefly to confirm normal response codes (200s).

\* Document the event in operational notes.



---



\## General Tips



\* \*\*To test alerts manually:\*\*



&nbsp; \* Stop the Blue app to trigger a failover:



&nbsp;   ```bash

&nbsp;   docker stop app\_blue

&nbsp;   ```

&nbsp; \* Restart it to trigger recovery:



&nbsp;   ```bash

&nbsp;   docker start app\_blue

&nbsp;   ```

&nbsp; \* Send several failing requests to trigger high error alerts.



\* \*\*To silence alerts temporarily:\*\*

&nbsp; Set the environment variable in `.env`:



&nbsp; ```

&nbsp; MAINTENANCE\_MODE=true

&nbsp; ```



&nbsp; Then restart the watcher:



&nbsp; ```bash

&nbsp; docker restart alert\_watcher

&nbsp; ```



\* \*\*To clear old alerts:\*\*

&nbsp; Delete Slack messages or reset the watcher container:



&nbsp; ```bash

&nbsp; docker restart alert\_watcher

&nbsp; ```



---



\## Summary



| Alert Type        | What It Means          | Typical Cause           | Operator Action                        |

| ----------------- | ---------------------- | ----------------------- | -------------------------------------- |

| Failover Detected | Nginx switched traffic | App crash or timeout    | Check failing pool and restart         |

| High Error Rate   | Too many 5xx errors    | App or dependency issue | Inspect logs and switch pool if needed |

| Recovery          | Primary app restored   | App came back online    | Verify health, no action needed        |



